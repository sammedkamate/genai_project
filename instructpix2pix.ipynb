{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UmzS9fzA7DY",
        "outputId": "c426762c-ba3a-40cb-ed05-b5c44155f02a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 112237, done.\u001b[K\n",
            "remote: Counting objects: 100% (373/373), done.\u001b[K\n",
            "remote: Compressing objects: 100% (247/247), done.\u001b[K\n",
            "remote: Total 112237 (delta 274), reused 126 (delta 126), pack-reused 111864 (from 3)\u001b[K\n",
            "Receiving objects: 100% (112237/112237), 86.00 MiB | 30.66 MiB/s, done.\n",
            "Resolving deltas: 100% (83631/83631), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xDnMu-XBNDQ",
        "outputId": "d0eac94a-30ec-4d0d-c33b-7ead7fd01ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/diffusers\n",
            "Processing /content/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers==0.36.0.dev0) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers==0.36.0.dev0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers==0.36.0.dev0) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.34.0->diffusers==0.36.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers==0.36.0.dev0) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.36.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers==0.36.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->diffusers==0.36.0.dev0) (1.3.1)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.36.0.dev0-py3-none-any.whl size=4521797 sha256=fedfffa34002627fc69fb35f447ab7dd815c3a780e8d742733f7d499f616cd94\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nq8menvi/wheels/8a/fc/09/385efb77b455b2fd4a656c950079c93147e1f50ae614e51beb\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.35.2\n",
            "    Uninstalling diffusers-0.35.2:\n",
            "      Successfully uninstalled diffusers-0.35.2\n",
            "Successfully installed diffusers-0.36.0.dev0\n"
          ]
        }
      ],
      "source": [
        "%cd diffusers\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5CG5C_JL5YX",
        "outputId": "04524274-64c8-4929-f4f8-d634d477edb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/diffusers/examples/instruct_pix2pix\n",
            "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.24.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.57.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (4.0.0)\n",
            "Collecting ftfy (from -r requirements.txt (line 5))\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.16.0->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 4)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->-r requirements.txt (line 5)) (0.2.14)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 6)) (3.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 3)) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.16.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "%cd examples/instruct_pix2pix\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyIVMOrIBRMs",
        "outputId": "5d2f43e3-1b2f-4485-d553-140a1bc1a414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w9S6cAN3mK7",
        "outputId": "1650b5f4-b44a-4939-8112-73ee063a1f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml1FTZ5aMyF_",
        "outputId": "fba2e23f-fa25-4c01-b2ad-9809a96f3b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-11-27 01:56:56.531639: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-27 01:56:56.549087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764208616.570782    4037 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764208616.577385    4037 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764208616.593945    4037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764208616.593971    4037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764208616.593974    4037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764208616.593976    4037 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-27 01:56:56.599033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "scheduler_config.json: 100% 308/308 [00:00<00:00, 2.17MB/s]\n",
            "{'sample_max_value', 'timestep_spacing', 'thresholding', 'prediction_type', 'variance_type', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 4.97MB/s]\n",
            "vocab.json: 1.06MB [00:00, 107MB/s]\n",
            "merges.txt: 525kB [00:00, 134MB/s]\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 5.05MB/s]\n",
            "config.json: 100% 617/617 [00:00<00:00, 4.78MB/s]\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:02<00:00, 219MB/s]\n",
            "config.json: 100% 547/547 [00:00<00:00, 3.98MB/s]\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:00<00:00, 439MB/s]\n",
            "{'scaling_factor', 'use_quant_conv', 'latents_mean', 'mid_block_add_attention', 'use_post_quant_conv', 'latents_std', 'shift_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "config.json: 100% 743/743 [00:00<00:00, 4.99MB/s]\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:04<00:00, 775MB/s]\n",
            "{'transformer_layers_per_block', 'dual_cross_attention', 'num_class_embeds', 'upcast_attention', 'mid_block_type', 'reverse_transformer_layers_per_block', 'attention_type', 'projection_class_embeddings_input_dim', 'time_embedding_dim', 'time_cond_proj_dim', 'addition_embed_type', 'encoder_hid_dim_type', 'num_attention_heads', 'class_embed_type', 'time_embedding_type', 'use_linear_projection', 'cross_attention_norm', 'time_embedding_act_fn', 'addition_embed_type_num_heads', 'resnet_out_scale_factor', 'dropout', 'only_cross_attention', 'mid_block_only_cross_attention', 'conv_in_kernel', 'timestep_post_act', 'addition_time_embed_dim', 'resnet_time_scale_shift', 'class_embeddings_concat', 'resnet_skip_time_act', 'encoder_hid_dim', 'conv_out_kernel'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "INFO:__main__:Initializing the InstructPix2Pix UNet from the pretrained UNet.\n",
            "README.md: 100% 585/585 [00:00<00:00, 5.44MB/s]\n",
            "data/train-00000-of-00001-476d66d1245615(…): 100% 417M/417M [00:08<00:00, 51.7MB/s]\n",
            "Generating train split: 100% 1000/1000 [00:01<00:00, 663.74 examples/s]\n",
            "INFO:__main__:***** Running training *****\n",
            "INFO:__main__:  Num examples = 1000\n",
            "INFO:__main__:  Num Epochs = 4\n",
            "INFO:__main__:  Instantaneous batch size per device = 4\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = 250\n",
            "Steps: 100% 250/250 [06:30<00:00,  1.63s/it, lr=5e-5, step_loss=0.145]\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 5.30MB/s]\n",
            "\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 3.42MB/s]\n",
            "\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:01,  4.06it/s]\u001b[A\n",
            "\n",
            "config.json: 4.72kB [00:00, 22.2MB/s]\n",
            "\n",
            "\n",
            "safety_checker/model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:   0% 67.8k/1.22G [00:02<11:40:04, 28.9kB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  17% 210M/1.22G [00:02<00:09, 111MB/s]     \u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors:  61% 738M/1.22G [00:02<00:01, 471MB/s]\u001b[A\u001b[A\n",
            "\n",
            "safety_checker/model.safetensors: 100% 1.22G/1.22G [00:02<00:00, 413MB/s]\n",
            "\n",
            "Fetching 9 files: 100% 9/9 [00:03<00:00,  2.63it/s]\n",
            "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:00<00:00, 37.37it/s]\n",
            "Configuration saved in /content/drive/MyDrive/genai/instructpix2pix/model/vae/config.json\n",
            "Model weights saved in /content/drive/MyDrive/genai/instructpix2pix/model/vae/diffusion_pytorch_model.safetensors\n",
            "Configuration saved in /content/drive/MyDrive/genai/instructpix2pix/model/unet/config.json\n",
            "Model weights saved in /content/drive/MyDrive/genai/instructpix2pix/model/unet/diffusion_pytorch_model.safetensors\n",
            "Configuration saved in /content/drive/MyDrive/genai/instructpix2pix/model/scheduler/scheduler_config.json\n",
            "Configuration saved in /content/drive/MyDrive/genai/instructpix2pix/model/model_index.json\n",
            "Steps: 100% 250/250 [06:50<00:00,  1.64s/it, lr=5e-5, step_loss=0.145]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# TRAINING 1: Pretrained Model (from stable-diffusion-v1-5)\n",
        "# This will be used as the \"background/prior\" model in guidance mechanism\n",
        "# ============================================================================\n",
        "\n",
        "!accelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\n",
        "    --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\"  \\\n",
        "    --dataset_name=\"fusing/instructpix2pix-1000-samples\" \\\n",
        "    --resolution=256 \\\n",
        "    --random_flip \\\n",
        "    --train_batch_size=4 \\\n",
        "    --gradient_accumulation_steps=4 \\\n",
        "    --gradient_checkpointing \\\n",
        "    --max_train_steps=250 \\\n",
        "    --checkpointing_steps=5000 \\\n",
        "    --checkpoints_total_limit=1 \\\n",
        "    --learning_rate=5e-05 \\\n",
        "    --max_grad_norm=1 \\\n",
        "    --lr_warmup_steps=0 \\\n",
        "    --conditioning_dropout_prob=0.05 \\\n",
        "    --mixed_precision=fp16 \\\n",
        "    --output_dir=\"/content/drive/MyDrive/genai/instructpix2pix/pretrained_model\" \\\n",
        "    --seed=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAINING 2: Trained Model (from customized fine-tuned Stable Diffusion)\n",
        "# This will be used as the \"fine-tuned\" model in guidance mechanism\n",
        "# ============================================================================\n",
        "\n",
        "# IMPORTANT: Replace this path with your customized fine-tuned Stable Diffusion model\n",
        "# This should be a standard Stable Diffusion model (4-channel UNet) that you've fine-tuned\n",
        "# Examples:\n",
        "#   - A DreamBooth model: \"path/to/your/dreambooth-model\"\n",
        "#   - A LoRA model converted to full weights: \"path/to/your/lora-model\"\n",
        "#   - Any other fine-tuned SD v1.5 model\n",
        "\n",
        "# IMPORTANT: Replace this path with your customized fine-tuned Stable Diffusion model\n",
        "CUSTOMIZED_SD_MODEL_PATH = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"  # TODO: Replace with your customized model path\n",
        "\n",
        "print(f\"Training InstructPix2Pix model from customized SD model: {CUSTOMIZED_SD_MODEL_PATH}\")\n",
        "print(\"Note: The training script will automatically convert it to InstructPix2Pix format (8-channel UNet)\")\n",
        "\n",
        "# Build the command with the custom model path\n",
        "train_cmd = f'''accelerate launch --mixed_precision=\"fp16\" train_instruct_pix2pix.py \\\\\n",
        "    --pretrained_model_name_or_path=\"{CUSTOMIZED_SD_MODEL_PATH}\"  \\\\\n",
        "    --dataset_name=\"fusing/instructpix2pix-1000-samples\" \\\\\n",
        "    --resolution=256 \\\\\n",
        "    --random_flip \\\\\n",
        "    --train_batch_size=4 \\\\\n",
        "    --gradient_accumulation_steps=4 \\\\\n",
        "    --gradient_checkpointing \\\\\n",
        "    --max_train_steps=250 \\\\\n",
        "    --checkpointing_steps=5000 \\\\\n",
        "    --checkpoints_total_limit=1 \\\\\n",
        "    --learning_rate=5e-05 \\\\\n",
        "    --max_grad_norm=1 \\\\\n",
        "    --lr_warmup_steps=0 \\\\\n",
        "    --conditioning_dropout_prob=0.05 \\\\\n",
        "    --mixed_precision=fp16 \\\\\n",
        "    --output_dir=\"/content/drive/MyDrive/genai/instructpix2pix/trained_model\" \\\\\n",
        "    --seed=42'''\n",
        "\n",
        "!{train_cmd}\n",
        "\n",
        "print(\"\\n✓ Trained model training complete!\")\n",
        "print(\"Saved to: /content/drive/MyDrive/genai/instructpix2pix/trained_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d12ff574e1bb4de482095ac18197b8f5",
            "321931f8b0a54f678e89e23f8e726ed1",
            "95e403ce2605497eaf6a17f293d89d23",
            "093b1aa5f65541cfab08cc2fa23ef4c4",
            "826c98300a764d68840d516b26248e24",
            "5f82b38461584679a3775c0f99354cde",
            "fdf1356a96814db0a4c51e9c192c5c02",
            "ff7232a2e5d94ec5a718a25619bbaede",
            "ba6ef9901ed8432d8b6549c19f13b4aa",
            "d99bcb0c9b7b483b848806568e168b0c",
            "ac2b852a4a7e4d5e87570709ed5e2f6e",
            "40a2172596244ccda0aff6f01fdca45b",
            "ce25c05217e54038ac35f8ca23dbeeb1",
            "2cd61af4ec93462db2ad49dc5a702128",
            "dc21fcbed61946769f37d1609fce6a70",
            "1c7fb5375869467798d84fe6c2b9df79",
            "70b9de62dd734264b242a89586c908a1",
            "94963d1d249d457c85e30197515704ea",
            "0aac82a55dc748eba3ee20c95b56c951",
            "bb388875878245d29131b02f3d1520a0",
            "3d3f2844b2a1488188ef982fcd4f294b",
            "00a7dc3dbccd4b71afaaed5a1f43e639"
          ]
        },
        "id": "R9Has0_MrZy3",
        "outputId": "cd5ebb52-19c6-4a4a-b182-b6ed1b131ce3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d12ff574e1bb4de482095ac18197b8f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40a2172596244ccda0aff6f01fdca45b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\"/content/drive/MyDrive/genai/instructpix2pix/model\", torch_dtype=torch.float16).to(\"cuda\")\n",
        "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
        "\n",
        "image = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\n",
        "prompt = \"add duck on the lake\"\n",
        "num_inference_steps = 20\n",
        "image_guidance_scale = 2\n",
        "guidance_scale = 8\n",
        "\n",
        "edited_image = pipeline(\n",
        "   prompt,\n",
        "   image=image,\n",
        "   num_inference_steps=num_inference_steps,\n",
        "   image_guidance_scale=image_guidance_scale,\n",
        "   guidance_scale=guidance_scale,\n",
        "   generator=generator,\n",
        ").images[0]\n",
        "edited_image.save(\"duck8_2.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BACKGROUND GUIDANCE FOR INSTRUCTPIX2PIX\n",
        "# Similar to bg_guidance.py but adapted for InstructPix2Pix pipeline\n",
        "# MEMORY OPTIMIZED VERSION - includes attention slicing, VAE slicing, and cache clearing\n",
        "# ============================================================================\n",
        "\n",
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "from diffusers.utils import load_image\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Clear any existing GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def generate_with_bg_guidance(\n",
        "    pipeline_trained,\n",
        "    pipeline_pretrained,\n",
        "    prompt,\n",
        "    image,\n",
        "    num_inference_steps=20,\n",
        "    image_guidance_scale=2.0,\n",
        "    guidance_scale=8.0,\n",
        "    omega=0.5,  # Interpolation weight: 0.0 = only pretrained, 1.0 = only trained\n",
        "    generator=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate image with background guidance that blends predictions from\n",
        "    pretrained and trained InstructPix2Pix models.\n",
        "\n",
        "    This is similar to bg_guidance.py but adapted for InstructPix2Pix.\n",
        "    At each denoising step, it blends the noise predictions from both models.\n",
        "    \n",
        "    MEMORY OPTIMIZED: This function includes memory optimizations:\n",
        "    - Explicit tensor deletion and cache clearing\n",
        "    - Sequential model processing\n",
        "    - torch.no_grad() context\n",
        "    - Assumes attention slicing and VAE slicing are enabled on pipelines\n",
        "\n",
        "    Args:\n",
        "        pipeline_trained: Fine-tuned InstructPix2Pix pipeline\n",
        "        pipeline_pretrained: Pretrained InstructPix2Pix pipeline\n",
        "        prompt: Edit instruction text\n",
        "        image: Input image to edit (PIL Image or tensor)\n",
        "        num_inference_steps: Number of denoising steps (reduce if OOM)\n",
        "        image_guidance_scale: Image guidance scale (for InstructPix2Pix)\n",
        "        guidance_scale: Text guidance scale (classifier-free guidance)\n",
        "        omega: Interpolation weight between models (0.0-1.0)\n",
        "               - 0.0: Use only pretrained model (more general, preserves background)\n",
        "               - 1.0: Use only trained model (more specific to training data)\n",
        "               - 0.5: Equal blend of both models\n",
        "        generator: Random generator for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        PIL Image: Generated edited image\n",
        "    \n",
        "    Note: If you still get OOM errors, try:\n",
        "    - Reducing image resolution (resize input image before calling)\n",
        "    - Reducing num_inference_steps\n",
        "    - Enabling CPU offloading on pipelines\n",
        "    - Processing with smaller batch sizes\n",
        "    \"\"\"\n",
        "    device = pipeline_trained.device\n",
        "    dtype = next(pipeline_trained.unet.parameters()).dtype\n",
        "    \n",
        "    # 1. Preprocess image\n",
        "    if isinstance(image, PIL.Image.Image):\n",
        "        image = pipeline_trained.image_processor.preprocess(image).to(device=device, dtype=dtype)\n",
        "    elif isinstance(image, torch.Tensor):\n",
        "        image = image.to(device=device, dtype=dtype)\n",
        "    \n",
        "    # 2. Encode prompt\n",
        "    do_classifier_free_guidance = guidance_scale > 1.0 or image_guidance_scale > 1.0\n",
        "    prompt_embeds = pipeline_trained._encode_prompt(\n",
        "        prompt,\n",
        "        device,\n",
        "        num_images_per_prompt=1,\n",
        "        do_classifier_free_guidance=do_classifier_free_guidance,\n",
        "        negative_prompt=None,\n",
        "    )\n",
        "    \n",
        "    # 3. Prepare image latents (using pipeline's method which handles CFG expansion)\n",
        "    batch_size = 1\n",
        "    num_images_per_prompt = 1\n",
        "    image_latents = pipeline_trained.prepare_image_latents(\n",
        "        image,\n",
        "        batch_size,\n",
        "        num_images_per_prompt,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        do_classifier_free_guidance,\n",
        "        generator=generator,\n",
        "    )\n",
        "    # image_latents is now [batch, batch, zeros_batch] if CFG, else [batch]\n",
        "    \n",
        "    # Get height/width from image latents (after CFG expansion, but dimensions are the same)\n",
        "    # These are in latent space, need to convert to pixel space for prepare_latents\n",
        "    height, width = image_latents.shape[-2:]\n",
        "    height = height * pipeline_trained.vae_scale_factor\n",
        "    width = width * pipeline_trained.vae_scale_factor\n",
        "    \n",
        "    # 4. Prepare timesteps\n",
        "    pipeline_trained.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "    timesteps = pipeline_trained.scheduler.timesteps\n",
        "    \n",
        "    # 5. Prepare noise latents (for the edited image)\n",
        "    # Note: latents are NOT expanded here - they're expanded in the loop\n",
        "    num_channels_latents = pipeline_trained.vae.config.latent_channels  # 4 channels\n",
        "    latents = pipeline_trained.prepare_latents(\n",
        "        batch_size * num_images_per_prompt,\n",
        "        num_channels_latents,\n",
        "        height,\n",
        "        width,\n",
        "        prompt_embeds.dtype,\n",
        "        device,\n",
        "        generator,\n",
        "        latents=None,\n",
        "    )\n",
        "    \n",
        "    # 6. Denoising loop with blended guidance (MEMORY OPTIMIZED)\n",
        "    num_warmup_steps = len(timesteps) - num_inference_steps * pipeline_trained.scheduler.order\n",
        "    \n",
        "    # Note: Memory optimizations (attention slicing, VAE slicing) should be enabled\n",
        "    # when loading the models, not here. This ensures they're set once.\n",
        "    \n",
        "    with torch.no_grad():  # Ensure no gradient computation\n",
        "        with pipeline_trained.progress_bar(total=num_inference_steps) as progress_bar:\n",
        "            for i, t in enumerate(timesteps):\n",
        "                # Expand latents for classifier-free guidance (3-way)\n",
        "                latent_model_input = torch.cat([latents] * 3) if do_classifier_free_guidance else latents\n",
        "                latent_model_input = pipeline_trained.scheduler.scale_model_input(latent_model_input, t)\n",
        "                \n",
        "                # Concatenate image latents (InstructPix2Pix uses 8-channel input: 4 latents + 4 image)\n",
        "                latent_model_input = torch.cat([latent_model_input, image_latents], dim=1)\n",
        "                \n",
        "                # Get noise predictions from both models (process sequentially to save memory)\n",
        "                # Trained model prediction\n",
        "                noise_pred_trained = pipeline_trained.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "                \n",
        "                # Clear cache before second model call\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                # Pretrained model prediction\n",
        "                noise_pred_pretrained = pipeline_pretrained.unet(\n",
        "                    latent_model_input,\n",
        "                    t,\n",
        "                    encoder_hidden_states=prompt_embeds,\n",
        "                    return_dict=False,\n",
        "                )[0]\n",
        "                \n",
        "                # Blend predictions using omega (similar to bg_guidance.py)\n",
        "                # omega=0.0: only pretrained, omega=1.0: only trained\n",
        "                noise_pred = omega * noise_pred_trained + (1.0 - omega) * noise_pred_pretrained\n",
        "                \n",
        "                # Delete intermediate tensors to free memory\n",
        "                del noise_pred_trained, noise_pred_pretrained\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                # Perform InstructPix2Pix-specific classifier-free guidance\n",
        "                # InstructPix2Pix uses 3-way guidance: text, image, unconditional\n",
        "                if do_classifier_free_guidance:\n",
        "                    noise_pred_text, noise_pred_image, noise_pred_uncond = noise_pred.chunk(3)\n",
        "                    noise_pred = (\n",
        "                        noise_pred_uncond\n",
        "                        + guidance_scale * (noise_pred_text - noise_pred_image)\n",
        "                        + image_guidance_scale * (noise_pred_image - noise_pred_uncond)\n",
        "                    )\n",
        "                    del noise_pred_text, noise_pred_image, noise_pred_uncond\n",
        "                \n",
        "                # Compute the previous noisy sample x_t -> x_t-1\n",
        "                latents = pipeline_trained.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
        "                \n",
        "                # Clean up\n",
        "                del noise_pred, latent_model_input\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                # Update progress bar\n",
        "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipeline_trained.scheduler.order == 0):\n",
        "                    progress_bar.update()\n",
        "    \n",
        "    # 7. Decode latents to image\n",
        "    with torch.no_grad():\n",
        "        image = pipeline_trained.vae.decode(\n",
        "            latents / pipeline_trained.vae.config.scaling_factor,\n",
        "            return_dict=False\n",
        "        )[0]\n",
        "        # Detach to remove gradient requirement before postprocessing\n",
        "        image = image.detach()\n",
        "        image = pipeline_trained.image_processor.postprocess(image, output_type=\"pil\")\n",
        "    \n",
        "    return image[0] if isinstance(image, list) else image\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Load models for background guidance\n",
        "# ============================================================================\n",
        "\n",
        "# Paths to the trained models\n",
        "pretrained_model_path = \"/content/drive/MyDrive/genai/instructpix2pix/pretrained_model\"  # From Training 1\n",
        "trained_model_path = \"/content/drive/MyDrive/genai/instructpix2pix/trained_model\"  # From Training 2\n",
        "\n",
        "print(\"Loading models for background guidance mechanism...\")\n",
        "print(f\"  - Pretrained model: {pretrained_model_path}\")\n",
        "print(f\"  - Trained model: {trained_model_path}\")\n",
        "\n",
        "# Load pretrained pipeline (from stable-diffusion-v1-5)\n",
        "print(\"\\nLoading pretrained model (background/prior model)...\")\n",
        "pretrained_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "    pretrained_model_path, \n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Load trained pipeline (from customized fine-tuned SD model)\n",
        "print(\"Loading trained model (fine-tuned model)...\")\n",
        "trained_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "    trained_model_path, \n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Enable memory optimizations for both pipelines\n",
        "print(\"\\nEnabling memory optimizations...\")\n",
        "pretrained_pipeline.enable_attention_slicing()\n",
        "trained_pipeline.enable_attention_slicing()\n",
        "pretrained_pipeline.enable_vae_slicing()\n",
        "trained_pipeline.enable_vae_slicing()\n",
        "\n",
        "# Optional: Use CPU offloading if memory is still an issue\n",
        "# Uncomment the following lines if you still get OOM errors:\n",
        "# print(\"Enabling CPU offloading (slower but uses less GPU memory)...\")\n",
        "# pretrained_pipeline.enable_model_cpu_offload()\n",
        "# trained_pipeline.enable_model_cpu_offload()\n",
        "\n",
        "print(\"\\n✓ Both models loaded successfully!\")\n",
        "\n",
        "# Validate that both models are InstructPix2Pix models (8-channel UNet)\n",
        "assert pretrained_pipeline.unet.config.in_channels == 8, \\\n",
        "    f\"Pretrained model must be InstructPix2Pix (8 channels), got {pretrained_pipeline.unet.config.in_channels} channels\"\n",
        "assert trained_pipeline.unet.config.in_channels == 8, \\\n",
        "    f\"Trained model must be InstructPix2Pix (8 channels), got {trained_pipeline.unet.config.in_channels} channels\"\n",
        "print(\"✓ Both models have compatible 8-channel UNet architecture\")\n",
        "print(f\"  - Pretrained model: {pretrained_pipeline.unet.config.in_channels} channels\")\n",
        "print(f\"  - Trained model: {trained_pipeline.unet.config.in_channels} channels\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Test the background guidance mechanism\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n=== Testing Background Guidance Method ===\\n\")\n",
        "generator = torch.Generator(\"cuda\").manual_seed(0)\n",
        "image = load_image(\"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\")\n",
        "prompt = \"add duck on the lake\"\n",
        "\n",
        "# Test with different omega values to see the effect\n",
        "omega_values = [0.0, 0.5, 1.0]\n",
        "for omega in omega_values:\n",
        "    print(f\"\\nGenerating with omega={omega} (omega=0.0: only pretrained, omega=1.0: only trained)...\")\n",
        "    edited_image = generate_with_bg_guidance(\n",
        "        pipeline_trained=trained_pipeline,\n",
        "        pipeline_pretrained=pretrained_pipeline,\n",
        "        prompt=prompt,\n",
        "        image=image,\n",
        "        num_inference_steps=20,\n",
        "        image_guidance_scale=2.0,\n",
        "        guidance_scale=8.0,\n",
        "        omega=omega,\n",
        "        generator=generator,\n",
        "    )\n",
        "    \n",
        "    filename = f\"duck_omega_{omega:.1f}.png\"\n",
        "    edited_image.save(filename)\n",
        "    print(f\"Saved: {filename}\")\n",
        "    \n",
        "    # Display the image\n",
        "    display(edited_image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00a7dc3dbccd4b71afaaed5a1f43e639": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "093b1aa5f65541cfab08cc2fa23ef4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99bcb0c9b7b483b848806568e168b0c",
            "placeholder": "​",
            "style": "IPY_MODEL_ac2b852a4a7e4d5e87570709ed5e2f6e",
            "value": " 7/7 [00:02&lt;00:00,  2.05it/s]"
          }
        },
        "0aac82a55dc748eba3ee20c95b56c951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c7fb5375869467798d84fe6c2b9df79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd61af4ec93462db2ad49dc5a702128": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aac82a55dc748eba3ee20c95b56c951",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb388875878245d29131b02f3d1520a0",
            "value": 20
          }
        },
        "321931f8b0a54f678e89e23f8e726ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f82b38461584679a3775c0f99354cde",
            "placeholder": "​",
            "style": "IPY_MODEL_fdf1356a96814db0a4c51e9c192c5c02",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "3d3f2844b2a1488188ef982fcd4f294b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a2172596244ccda0aff6f01fdca45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce25c05217e54038ac35f8ca23dbeeb1",
              "IPY_MODEL_2cd61af4ec93462db2ad49dc5a702128",
              "IPY_MODEL_dc21fcbed61946769f37d1609fce6a70"
            ],
            "layout": "IPY_MODEL_1c7fb5375869467798d84fe6c2b9df79"
          }
        },
        "5f82b38461584679a3775c0f99354cde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b9de62dd734264b242a89586c908a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "826c98300a764d68840d516b26248e24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94963d1d249d457c85e30197515704ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95e403ce2605497eaf6a17f293d89d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7232a2e5d94ec5a718a25619bbaede",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba6ef9901ed8432d8b6549c19f13b4aa",
            "value": 7
          }
        },
        "ac2b852a4a7e4d5e87570709ed5e2f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba6ef9901ed8432d8b6549c19f13b4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb388875878245d29131b02f3d1520a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce25c05217e54038ac35f8ca23dbeeb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70b9de62dd734264b242a89586c908a1",
            "placeholder": "​",
            "style": "IPY_MODEL_94963d1d249d457c85e30197515704ea",
            "value": "100%"
          }
        },
        "d12ff574e1bb4de482095ac18197b8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_321931f8b0a54f678e89e23f8e726ed1",
              "IPY_MODEL_95e403ce2605497eaf6a17f293d89d23",
              "IPY_MODEL_093b1aa5f65541cfab08cc2fa23ef4c4"
            ],
            "layout": "IPY_MODEL_826c98300a764d68840d516b26248e24"
          }
        },
        "d99bcb0c9b7b483b848806568e168b0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc21fcbed61946769f37d1609fce6a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d3f2844b2a1488188ef982fcd4f294b",
            "placeholder": "​",
            "style": "IPY_MODEL_00a7dc3dbccd4b71afaaed5a1f43e639",
            "value": " 20/20 [00:01&lt;00:00, 11.06it/s]"
          }
        },
        "fdf1356a96814db0a4c51e9c192c5c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7232a2e5d94ec5a718a25619bbaede": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
